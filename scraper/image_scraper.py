"""
ç½‘ç«™å›¾ç‰‡çˆ¬è™«å·¥å…·
ç”¨äºä»æŒ‡å®šç½‘é¡µçˆ¬å–æ‰€æœ‰å›¾ç‰‡å¹¶ä¸‹è½½åˆ°æœ¬åœ°

ä½¿ç”¨æ–¹æ³•:
    python image_scraper.py <ç½‘é¡µURL>
    
ç¤ºä¾‹:
    python image_scraper.py http://www.yudongguolu.com/?SortId=8&Type=list
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
import sys
from pathlib import Path
import time

class ImageScraper:
    def __init__(self, output_dir="downloaded_images"):
        """
        åˆå§‹åŒ–å›¾ç‰‡çˆ¬è™«
        
        Args:
            output_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
        """
        self.output_dir = output_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
    def get_page_content(self, url):
        """
        è·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            BeautifulSoupå¯¹è±¡
        """
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"âŒ è®¿é—®ç½‘é¡µå¤±è´¥: {e}")
            return None
    
    def extract_image_urls(self, soup, base_url):
        """
        ä»ç½‘é¡µä¸­æå–æ‰€æœ‰å›¾ç‰‡URL
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            base_url: åŸºç¡€URL,ç”¨äºæ‹¼æ¥ç›¸å¯¹è·¯å¾„
            
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        image_urls = []
        
        # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
        for img in soup.find_all('img'):
            img_url = img.get('src') or img.get('data-src') or img.get('data-original')
            if img_url:
                # è½¬æ¢ä¸ºç»å¯¹URL
                full_url = urljoin(base_url, img_url)
                image_urls.append(full_url)
        
        # å»é‡
        image_urls = list(set(image_urls))
        print(f"âœ… æ‰¾åˆ° {len(image_urls)} å¼ å›¾ç‰‡")
        return image_urls
    
    def download_image(self, url, filename):
        """
        ä¸‹è½½å•å¼ å›¾ç‰‡
        
        Args:
            url: å›¾ç‰‡URL
            filename: ä¿å­˜çš„æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            response = self.session.get(url, timeout=10, stream=True)
            response.raise_for_status()
            
            filepath = os.path.join(self.output_dir, filename)
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(filepath) / 1024  # KB
            print(f"  âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size:.1f} KB)")
            return True
            
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {filename} - {e}")
            return False
    
    def get_filename_from_url(self, url, index):
        """
        ä»URLç”Ÿæˆæ–‡ä»¶å
        
        Args:
            url: å›¾ç‰‡URL
            index: å›¾ç‰‡ç´¢å¼•
            
        Returns:
            æ–‡ä»¶å
        """
        # å°è¯•ä»URLè·å–æ–‡ä»¶å
        parsed = urlparse(url)
        path = parsed.path
        
        if path:
            filename = os.path.basename(path)
            # å¦‚æœæ–‡ä»¶åæœ‰æ•ˆä¸”æœ‰æ‰©å±•å
            if filename and '.' in filename:
                return filename
        
        # å¦åˆ™ä½¿ç”¨ç´¢å¼•å’Œé»˜è®¤æ‰©å±•å
        ext = '.jpg'  # é»˜è®¤æ‰©å±•å
        if '.png' in url.lower():
            ext = '.png'
        elif '.gif' in url.lower():
            ext = '.gif'
        elif '.webp' in url.lower():
            ext = '.webp'
            
        return f"image_{index:03d}{ext}"
    
    def scrape(self, url):
        """
        çˆ¬å–æŒ‡å®šç½‘é¡µçš„æ‰€æœ‰å›¾ç‰‡
        
        Args:
            url: ç½‘é¡µURL
        """
        print("=" * 60)
        print("ğŸš€ å¼€å§‹çˆ¬å–å›¾ç‰‡...")
        print("=" * 60)
        
        # è·å–ç½‘é¡µå†…å®¹
        soup = self.get_page_content(url)
        if not soup:
            return
        
        # æå–å›¾ç‰‡URL
        image_urls = self.extract_image_urls(soup, url)
        
        if not image_urls:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
            return
        
        # ä¸‹è½½å›¾ç‰‡
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½å›¾ç‰‡åˆ°: {self.output_dir}")
        print("-" * 60)
        
        success_count = 0
        for i, img_url in enumerate(image_urls, 1):
            filename = self.get_filename_from_url(img_url, i)
            print(f"[{i}/{len(image_urls)}] {img_url}")
            
            if self.download_image(img_url, filename):
                success_count += 1
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        print("-" * 60)
        print(f"âœ¨ å®Œæˆ! æˆåŠŸä¸‹è½½ {success_count}/{len(image_urls)} å¼ å›¾ç‰‡")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {os.path.abspath(self.output_dir)}")
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python image_scraper.py <ç½‘é¡µURL>")
        print("ç¤ºä¾‹: python image_scraper.py http://www.yudongguolu.com/?SortId=8&Type=list")
        sys.exit(1)
    
    url = sys.argv[1]
    
    # å¯é€‰: è‡ªå®šä¹‰è¾“å‡ºç›®å½•
    output_dir = sys.argv[2] if len(sys.argv) > 2 else "downloaded_images"
    
    # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
    scraper = ImageScraper(output_dir=output_dir)
    scraper.scrape(url)


if __name__ == "__main__":
    main()
