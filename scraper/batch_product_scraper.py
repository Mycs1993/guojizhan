"""
æ‰¹é‡äº§å“çˆ¬è™« - è‡ªåŠ¨æŠ“å–å›½å†…ç«™æ‰€æœ‰äº§å“å¹¶ç”Ÿæˆ products.ts ä»£ç 
================================================================

ç”¨é€”ï¼š
    ä»å›½å†…ç«™çš„äº§å“åˆ—è¡¨é¡µå¼€å§‹ï¼Œè‡ªåŠ¨æŠ“å–æ‰€æœ‰äº§å“è¯¦æƒ…é¡µï¼Œæå–äº§å“ä¿¡æ¯ï¼Œ
    å¹¶è‡ªåŠ¨ç”Ÿæˆç¬¦åˆå›½é™…ç«™ products.ts æ ¼å¼çš„ TypeScript ä»£ç ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # æŠ“å–æŸä¸ªåˆ†ç±»ä¸‹çš„æ‰€æœ‰äº§å“ï¼ˆä¾‹å¦‚ï¼šç‡ƒæ²¹æ°”é”…ç‚‰åˆ†ç±»ï¼‰
    python batch_product_scraper.py "http://www.yudongguolu.com/?SortId=10&Type=list"
    
    # æˆ–è€…æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
    python batch_product_scraper.py "http://www.yudongguolu.com/?SortId=10&Type=list" output.ts

è¾“å‡ºï¼š
    ä¼šç”Ÿæˆä¸€ä¸ª TypeScript æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰äº§å“çš„å®Œæ•´é…ç½®ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶åˆ° products.ts é‡Œã€‚
"""

import re
import sys
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse, parse_qs

import requests
from bs4 import BeautifulSoup


BASE_URL = "http://www.yudongguolu.com"
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
})


def fetch_html(url: str) -> BeautifulSoup:
    """è·å–å¹¶è§£æç½‘é¡µ HTML"""
    print(f"ğŸ“¥ æ­£åœ¨è®¿é—®: {url}")
    resp = session.get(url, timeout=15)
    resp.raise_for_status()
    resp.encoding = resp.apparent_encoding or "utf-8"
    return BeautifulSoup(resp.text, "html.parser")


def extract_product_links(list_url: str) -> list[str]:
    """ä»äº§å“åˆ—è¡¨é¡µæå–æ‰€æœ‰äº§å“è¯¦æƒ…é¡µé“¾æ¥"""
    soup = fetch_html(list_url)
    links = []
    
    # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å‘äº§å“è¯¦æƒ…é¡µçš„é“¾æ¥ï¼ˆé€šå¸¸åŒ…å« Type=page&Id=ï¼‰
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "Type=page" in href and "Id=" in href:
            full_url = urljoin(BASE_URL, href)
            if full_url not in links:
                links.append(full_url)
    
    print(f"âœ… æ‰¾åˆ° {len(links)} ä¸ªäº§å“é“¾æ¥")
    return links


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€HTMLå®ä½“ç­‰"""
    if not text:
        return ""
    # æ›¿æ¢ HTML å®ä½“
    text = text.replace("&nbsp;", " ").replace("&ldquo;", '"').replace("&rdquo;", '"')
    text = text.replace("&lsquo;", "'").replace("&rsquo;", "'")
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_product_info(product_url: str) -> dict:
    """ä»äº§å“è¯¦æƒ…é¡µæå–äº§å“ä¿¡æ¯"""
    soup = fetch_html(product_url)
    
    # 1. äº§å“åç§°ï¼ˆä» h1 æˆ– titleï¼‰
    name_zh = ""
    h1 = soup.find("h1")
    if h1:
        name_zh = clean_text(h1.get_text())
    if not name_zh:
        title = soup.find("title")
        if title:
            name_zh = clean_text(title.get_text()).replace("-å¤ªåº·å¿è±«ä¸œé”…ç‚‰æœ‰é™å…¬å¸", "").strip()
    
    # 2. äº§å“ç®€ä»‹ï¼ˆä»äº§å“ç®€ä»‹åŒºåŸŸï¼‰
    intro_zh = ""
    intro_div = soup.find("div", style=lambda x: x and "height:78px" in x)
    if intro_div:
        intro_text = intro_div.get_text()
        if "äº§å“ç®€ä»‹ï¼š" in intro_text:
            intro_zh = clean_text(intro_text.split("äº§å“ç®€ä»‹ï¼š", 1)[-1])
    
    # 3. å®Œæ•´æè¿°ï¼ˆä»"äº§å“ä»‹ç»>>"éƒ¨åˆ†ï¼‰
    full_desc_zh = ""
    intro_section = soup.find("h2", string=lambda x: x and "äº§å“ä»‹ç»" in str(x))
    if intro_section:
        next_div = intro_section.find_next("div")
        if next_div:
            full_desc_zh = clean_text(next_div.get_text())
    
    # 4. æ€§èƒ½ä¼˜åŠ¿ï¼ˆä»"æ€§èƒ½ä¼˜åŠ¿>>"éƒ¨åˆ†ï¼‰
    features_zh = []
    advan_section = soup.find("h2", string=lambda x: x and "æ€§èƒ½ä¼˜åŠ¿" in str(x))
    if advan_section:
        next_div = advan_section.find_next("div")
        if next_div:
            # æå–æ‰€æœ‰å¸¦ç¼–å·çš„ä¼˜åŠ¿ç‚¹ï¼ˆä¾‹å¦‚ï¼š"ï¼ˆ1ï¼‰..."ï¼‰
            text = next_div.get_text()
            for match in re.finditer(r'ï¼ˆ(\d+)ï¼‰([^ï¼ˆ]+?)(?=ï¼ˆ\d+ï¼‰|$)', text):
                feature = clean_text(match.group(2))
                if feature:
                    features_zh.append(feature)
    
    # 5. æŠ€æœ¯å‚æ•°è¡¨
    specs = []
    param_section = soup.find("h2", string=lambda x: x and "æŠ€æœ¯å‚æ•°" in str(x))
    if param_section:
        table = param_section.find_next("table")
        if table:
            rows = []
            for tr in table.find_all("tr"):
                cells = [clean_text(td.get_text()) for td in tr.find_all(["td", "th"])]
                if any(cells):
                    rows.append(cells)
            
            # è§£æå‚æ•°è¡¨ï¼šç¬¬ä¸€è¡Œæ˜¯å‹å·ï¼Œåç»­è¡Œæ˜¯å‚æ•°
            if len(rows) > 1:
                # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´ï¼ˆå‹å·ï¼‰
                model_row = rows[0]
                # åç»­è¡Œæ˜¯å‚æ•°å€¼
                for row in rows[1:]:
                    if len(row) > 1 and len(row) == len(model_row):
                        # å‚æ•°å
                        param_name = row[0]
                        # ä¸ºæ¯ä¸ªå‹å·åˆ›å»ºä¸€ä¸ª specï¼ˆç®€åŒ–ç‰ˆï¼Œåªæå–å…³é”®å‚æ•°ï¼‰
                        for i, model in enumerate(model_row[1:], 1):
                            if i < len(row):
                                value = row[i]
                                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ ¹æ®å‚æ•°ååˆ†ç±»
                                if "é¢å®š" in param_name and ("åŠŸç‡" in param_name or "çƒ­åŠŸç‡" in param_name):
                                    if not specs or len(specs) < len(model_row) - 1:
                                        specs = [{"model": m, "capacity": "", "pressure": "", "efficiency": "", "fuel": ""} 
                                                for m in model_row[1:]]
                                    idx = i - 1
                                    if idx < len(specs):
                                        specs[idx]["capacity"] = value
                                elif "å‹åŠ›" in param_name:
                                    idx = i - 1
                                    if idx < len(specs):
                                        specs[idx]["pressure"] = value
                                elif "æ•ˆç‡" in param_name:
                                    idx = i - 1
                                    if idx < len(specs):
                                        specs[idx]["efficiency"] = value
    
    # 6. å›¾ç‰‡
    image_url = ""
    img = soup.find("div", class_="cppics")
    if img:
        img_tag = img.find("img")
        if img_tag and img_tag.get("src"):
            image_url = urljoin(BASE_URL, img_tag["src"])
    
    # 7. ä» URL æå–äº§å“ ID
    parsed = urlparse(product_url)
    params = parse_qs(parsed.query)
    product_id = params.get("Id", [""])[0]
    
    # ç”Ÿæˆè‹±æ–‡åç§°ï¼ˆç®€å•ç¿»è¯‘ï¼Œä½ å¯ä»¥åç»­æ‰‹åŠ¨ä¼˜åŒ–ï¼‰
    name_en = name_zh.replace("WNS", "WNS").replace("ä½æ°®", "Low-NOx").replace("å†·å‡", "Condensing")
    name_en = name_en.replace("ç‡ƒæ°”", "Gas").replace("çƒ­æ°´", "Hot Water").replace("é”…ç‚‰", "Boiler")
    name_en = name_en.replace("è’¸æ±½", "Steam").replace("å§å¼", "Horizontal").replace("ç«‹å¼", "Vertical")
    
    return {
        "id": f"product-{product_id}",
        "name_zh": name_zh,
        "name_en": name_en,
        "intro_zh": intro_zh,
        "full_desc_zh": full_desc_zh,
        "features_zh": features_zh,
        "specs": specs if specs else [{"model": "Custom", "capacity": "", "pressure": "", "efficiency": "", "fuel": ""}],
        "image_url": image_url
    }


def generate_ts_code(products: list[dict]) -> str:
    """ç”Ÿæˆ TypeScript ä»£ç """
    lines = []
    
    for i, p in enumerate(products):
        # äº§å“å¯¹è±¡å¼€å§‹
        lines.append("  {")
        lines.append(f'    id: "{p["id"]}",')
        lines.append("    name: {")
        lines.append(f'      en: "{p["name_en"]}",')
        lines.append(f'      zh: "{p["name_zh"]}"')
        lines.append("    },")
        lines.append("    description: {")
        lines.append(f'      en: "{p["name_en"]}. High efficiency and reliable operation.",')
        lines.append(f'      zh: "{p["intro_zh"][:100] if p["intro_zh"] else p["name_zh"]}..."')
        lines.append("    },")
        lines.append("    fullDescription: {")
        lines.append(f'      en: "{p["full_desc_zh"][:200] if p["full_desc_zh"] else p["intro_zh"][:200] if p["intro_zh"] else ""}...",')
        lines.append(f'      zh: "{p["full_desc_zh"][:300] if p["full_desc_zh"] else p["intro_zh"][:300] if p["intro_zh"] else ""}"')
        lines.append("    },")
        lines.append(f'    image: "{p["image_url"] or "/images/products/boiler.png"}",')
        lines.append('    icon: "Flame",')
        lines.append("    features: [")
        
        # æ€§èƒ½ä¼˜åŠ¿
        for feat in p["features_zh"][:5]:  # æœ€å¤š5ä¸ª
            feat_en = feat.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
            lines.append("      {")
            lines.append(f'        en: "{feat_en}",')
            lines.append(f'        zh: "{feat}"')
            lines.append("      },")
        
        lines.append("    ],")
        lines.append("    specs: [")
        
        # è§„æ ¼å‚æ•°
        for spec in p["specs"]:
            lines.append("      {")
            lines.append(f'        model: "{spec["model"]}",')
            lines.append(f'        capacity: "{spec["capacity"]}",')
            lines.append(f'        pressure: "{spec["pressure"]}",')
            lines.append(f'        efficiency: "{spec["efficiency"]}",')
            lines.append(f'        fuel: "{spec["fuel"]}"')
            lines.append("      },")
        
        lines.append("    ]")
        lines.append("  },")
        lines.append("")
    
    return "\n".join(lines)


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼špython batch_product_scraper.py <äº§å“åˆ—è¡¨é¡µURL> [è¾“å‡ºæ–‡ä»¶å]")
        print("ç¤ºä¾‹ï¼špython batch_product_scraper.py \"http://www.yudongguolu.com/?SortId=10&Type=list\"")
        sys.exit(1)
    
    list_url = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else "products_output.ts"
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ‰¹é‡æŠ“å–äº§å“...")
    print("=" * 60)
    
    # 1. è·å–æ‰€æœ‰äº§å“é“¾æ¥
    product_links = extract_product_links(list_url)
    
    if not product_links:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•äº§å“é“¾æ¥ï¼Œè¯·æ£€æŸ¥åˆ—è¡¨é¡µ URL")
        sys.exit(1)
    
    # 2. é€ä¸ªæŠ“å–äº§å“ä¿¡æ¯
    products = []
    for i, link in enumerate(product_links, 1):
        print(f"\n[{i}/{len(product_links)}] å¤„ç†äº§å“...")
        try:
            product_info = extract_product_info(link)
            products.append(product_info)
            print(f"  âœ… {product_info['name_zh']}")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
            continue
    
    # 3. ç”Ÿæˆ TypeScript ä»£ç 
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆ TypeScript ä»£ç ...")
    print("=" * 60)
    
    ts_code = generate_ts_code(products)
    
    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    output_path = Path(output_file)
    with output_path.open("w", encoding="utf-8") as f:
        f.write("// è‡ªåŠ¨ç”Ÿæˆçš„äº§å“æ•°æ®ï¼Œè¯·å¤åˆ¶åˆ° products.ts çš„ PRODUCT_CATEGORIES æ•°ç»„ä¸­\n")
        f.write("// æ³¨æ„ï¼šè‹±æ–‡æè¿°å’Œéƒ¨åˆ†å­—æ®µå¯èƒ½éœ€è¦æ‰‹åŠ¨ä¼˜åŒ–\n\n")
        f.write(ts_code)
    
    print(f"\nâœ… å®Œæˆï¼å·²æŠ“å– {len(products)} ä¸ªäº§å“")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_path.resolve()}")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. æ‰“å¼€ç”Ÿæˆçš„æ–‡ä»¶ï¼Œæ£€æŸ¥å¹¶ä¼˜åŒ–è‹±æ–‡æè¿°")
    print("   2. å¤åˆ¶ä»£ç åˆ° src/data/products.ts çš„ PRODUCT_CATEGORIES æ•°ç»„ä¸­")
    print("=" * 60)


if __name__ == "__main__":
    main()

